{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col  # For column operations in Spark DataFrames\n",
    "from pyspark.sql.types import TimestampType  # For casting columns to timestamp type\n",
    "\n",
    "# ---------------------------\n",
    "# Silver Layer: Cleansing and Structuring Earthquake Data\n",
    "# ---------------------------\n",
    "\n",
    "# Read the raw earthquake data (JSON) from the bronze layer into a Spark DataFrame.\n",
    "# - The file path uses the start_date variable for partitioning and traceability.\n",
    "# - The 'multiline' option is set to true to correctly parse JSON arrays.\n",
    "df = spark.read.option(\"multiline\", \"true\").json(f\"Files/{start_date}_earthquake_data.json\")\n",
    "\n",
    "# Reshape earthquake data by extracting and renaming key attributes for further analysis.\n",
    "# - Selects relevant fields from the nested JSON structure.\n",
    "# - Renames columns for clarity and downstream analytics.\n",
    "df = (\n",
    "    df\n",
    "    .select(\n",
    "        'id',\n",
    "        col('geometry.coordinates').getItem(0).alias('longitude'),\n",
    "        col('geometry.coordinates').getItem(1).alias('latitude'),\n",
    "        col('geometry.coordinates').getItem(2).alias('elevation'),\n",
    "        col('properties.title').alias('title'),\n",
    "        col('properties.place').alias('place_description'),\n",
    "        col('properties.sig').alias('sig'),\n",
    "        col('properties.mag').alias('mag'),\n",
    "        col('properties.magType').alias('magType'),\n",
    "        col('properties.time').alias('time'),\n",
    "        col('properties.updated').alias('updated')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Convert 'time' and 'updated' columns from milliseconds to timestamp format for clearer datetime representation.\n",
    "# - USGS API provides these as epoch milliseconds; Spark expects seconds for timestamp conversion.\n",
    "df = (\n",
    "    df\n",
    "    .withColumn('time', col('time')/1000)\n",
    "    .withColumn('updated', col('updated')/1000)\n",
    "    .withColumn('time', col('time').cast(TimestampType()))\n",
    "    .withColumn('updated', col('updated').cast(TimestampType()))\n",
    ")\n",
    "\n",
    "# Write the cleansed and structured data to the silver table.\n",
    "# - Uses 'append' mode to add new data without overwriting existing records.\n",
    "# - The table 'earthquake_events_silver' serves as the silver layer in the medallion architecture.\n",
    "df.write.mode('append').saveAsTable('earthquake_events_silver')\n",
    "\n",
    "# ---------------------------\n",
    "# Additional Info:\n",
    "# - The silver layer focuses on cleaning, normalizing, and structuring the raw data for analytics.\n",
    "# - Only relevant columns are kept, and timestamps are converted for easier querying.\n",
    "# - The resulting table can be used for reporting, dashboards, or further enrichment in the gold layer.\n",
    "# - Ensure the Spark session is available and the input file exists before running"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7132a30-2b1d-4341-a7b6-795a586ddee0",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Worldwide Earthquake Events API - Silver Layer Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3e2bd-9754-41dc-b1ed-2664deff749d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa465b-ded7-44bb-8659-133c553609e9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# df now is a Spark DataFrame containing JSON data\n",
    "df = spark.read.option(\"multiline\", \"true\").json(f\"Files/{start_date}_earthquake_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864a763-0ab0-47c5-a76e-0e196c9a0ca4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Reshape earthquake data by extracting and renaming key attributes for further analysis.\n",
    "df = \\\n",
    "df.\\\n",
    "    select(\n",
    "        'id',\n",
    "        col('geometry.coordinates').getItem(0).alias('longitude'),\n",
    "        col('geometry.coordinates').getItem(1).alias('latitude'),\n",
    "        col('geometry.coordinates').getItem(2).alias('elevation'),\n",
    "        col('properties.title').alias('title'),\n",
    "        col('properties.place').alias('place_description'),\n",
    "        col('properties.sig').alias('sig'),\n",
    "        col('properties.mag').alias('mag'),\n",
    "        col('properties.magType').alias('magType'),\n",
    "        col('properties.time').alias('time'),\n",
    "        col('properties.updated').alias('updated')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd86819c-d43f-41ac-9e4e-394a6592f0f6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Convert 'time' and 'updated' columns from milliseconds to timestamp format for clearer datetime representation.\n",
    "df = df.\\\n",
    "    withColumn('time', col('time')/1000).\\\n",
    "    withColumn('updated', col('updated')/1000).\\\n",
    "    withColumn('time', col('time').cast(TimestampType())).\\\n",
    "    withColumn('updated', col('updated').cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed7950e-b338-470d-aebc-60da0701d9ee",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# appending the data to the gold table\n",
    "df.write.mode('append').saveAsTable('earthquake_events_silver')"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
